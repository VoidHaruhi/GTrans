{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      "Namespace(dataset='ogb-arxiv', debug=1, dropout=0, epochs=10, existing_space=1, finetune=0, gpu_id=0, hidden=32, loop_adj=1, loop_feat=4, loss='LC', lr=0.01, lr_adj=0.1, lr_feat=0.001, margin=-1, model='GCN', nlayers=5, normalize_features=True, ood=1, ratio=0.1, seed=0, strategy='dropedge', tent=0, test_val=0, tune=0, weight_decay=0, with_bn=1)\n",
      "Namespace(K=3, T=1, beta=1.0, cached=False, cpu=False, data_dir='../data', dataset='ogb-arxiv', device=0, directed=False, display_step=100, dropout=0.0, epochs=500, gat_heads=2, gnn='gcn', gpr_alpha=0.1, hidden_channels=32, lp_alpha=0.1, lr=0.01, lr_a=0.005, method='erm', no_bn=False, num_layers=5, num_sample=5, rocauc=False, runs=5, sub_dataset='', weight_decay=0.0)\n",
      "Train num nodes 17401 | target nodes 17401 | num classes 40 | num node feats 128\n",
      "Val num nodes 41125 | target nodes 23724 | num classes 40 | num node feats 128\n",
      "GCN(\n",
      "  (layers): ModuleList(\n",
      "    (0): GCNConv(128, 32)\n",
      "    (1): GCNConv(32, 32)\n",
      "    (2): GCNConv(32, 32)\n",
      "    (3): GCNConv(32, 32)\n",
      "    (4): GCNConv(32, 40)\n",
      "  )\n",
      "  (bns): ModuleList(\n",
      "    (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Test accs: [0.4077676746317051, 0.39058566382389104, 0.348785054420509]\n",
      "flatten test: 0.37854279430345195\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "from gtransform_both import GraphAgent\n",
    "from utils import *\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "\n",
    "path = 'GraphOOD-EERM/'\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu_id', type=int, default=0, help='gpu id')\n",
    "parser.add_argument('--dataset', type=str, default='ogb-arxiv')\n",
    "parser.add_argument('--epochs', type=int, default=50)\n",
    "parser.add_argument('--hidden', type=int, default=32)\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4)\n",
    "parser.add_argument('--normalize_features', type=bool, default=True)\n",
    "parser.add_argument('--seed', type=int, default=0, help='Random seed.')\n",
    "parser.add_argument('--lr', type=float, default=0.01)\n",
    "parser.add_argument('--lr_feat', type=float, default=0.001)\n",
    "parser.add_argument('--nlayers', type=int, default=5)\n",
    "parser.add_argument('--model', type=str, default='GCN')\n",
    "parser.add_argument('--loss', type=str, default='LC')\n",
    "parser.add_argument('--debug', type=int, default=1)\n",
    "parser.add_argument('--ood', type=int, default=1)\n",
    "parser.add_argument('--with_bn', type=int, default=1)\n",
    "parser.add_argument('--lr_adj', type=float, default=0.1)\n",
    "parser.add_argument('--ratio', type=float, default=0.1, help='budget B for changing graph structure')\n",
    "parser.add_argument('--margin', type=float, default=-1)\n",
    "parser.add_argument('--existing_space', type=int, default=1, help='enable removing edges from the graph')\n",
    "parser.add_argument('--loop_adj', type=int, default=1, help='#loops for optimizing structure')\n",
    "parser.add_argument('--loop_feat', type=int, default=4, help='#loops for optimizing features')\n",
    "parser.add_argument('--test_val', type=int, default=0, help='set to 1 to evaluate performance on validation data')\n",
    "parser.add_argument('--tune', type=int, default=0)\n",
    "parser.add_argument('--finetune', type=int, default=0, help='whether to finetune the model')\n",
    "parser.add_argument('--tent', type=int, default=0, help='use the Tent for finetuning (need to set finetune=1)')\n",
    "parser.add_argument('--strategy', type=str, default='dropedge')\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "torch.cuda.set_device(args.gpu_id)\n",
    "\n",
    "lr_feat = args.lr_feat; epochs = args.epochs; ratio = args.ratio; lr_adj = args.lr_adj\n",
    "print('===========')\n",
    "reset_args(args)\n",
    "print(args)\n",
    "if args.model == 'GAT':\n",
    "    args.loop_adj = 0; args.loop_feat = args.epochs\n",
    "if args.tune: # set args.tune to 1 to change the model hyperparameters\n",
    "    args.lr_feat = lr_feat; args.epochs = epochs; args.ratio = ratio; args.lr_adj = lr_adj\n",
    "if args.epochs == 2:\n",
    "    args.loop_adj = 1; args.loop_feat = 1\n",
    "\n",
    "path = 'GraphOOD-EERM/'\n",
    "if args.dataset == 'elliptic':\n",
    "    path = path + 'temp_elliptic'\n",
    "    sys.path.append(path)\n",
    "    from main_as_utils import datasets_tr, datasets_val, datasets_te\n",
    "    data = [datasets_tr, datasets_val, datasets_te]\n",
    "elif args.dataset == 'fb100':\n",
    "    path = path + 'multigraph'\n",
    "    sys.path.append(path)\n",
    "    from main_as_utils_fb import datasets_tr, datasets_val, datasets_te\n",
    "    data = [datasets_tr, datasets_val, datasets_te]\n",
    "elif args.dataset == 'amazon-photo':\n",
    "    path = path + 'synthetic'\n",
    "    sys.path.append(path)\n",
    "    from main_as_utils_photo import dataset_tr, dataset_val, datasets_te\n",
    "    data = [dataset_tr, dataset_val, datasets_te]\n",
    "else:\n",
    "    if args.dataset == 'cora':\n",
    "        path = path + 'synthetic'\n",
    "    elif args.dataset == 'ogb-arxiv':\n",
    "        path = path + 'temp_arxiv'\n",
    "    elif args.dataset == 'twitch-e':\n",
    "        path = path + 'multigraph'\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    sys.path.append(path)\n",
    "    from main_as_utils import dataset_tr, dataset_val, datasets_te\n",
    "    data = [dataset_tr, dataset_val, datasets_te]\n",
    "# random seed setting\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "res = []\n",
    "agent = GraphAgent(data, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mean entropy: tensor(1.7533, device='cuda:0')\n",
      "Test mean entropy: [tensor(1.9408, device='cuda:0'), tensor(1.9609, device='cuda:0'), tensor(2.0648, device='cuda:0')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66792/3985302898.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(agent.model.predict(data[0].graph['node_feat'].to(agent.device), data[0].graph['edge_index'].to(agent.device))).detach()\n",
      "/tmp/ipykernel_66792/3985302898.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred_temp = F.softmax(agent.model.predict(data_test.graph['node_feat'].to(agent.device), data_test.graph['edge_index'].to(agent.device))).detach()\n"
     ]
    }
   ],
   "source": [
    "# preliminary study\n",
    "pred = F.softmax(agent.model.predict(data[0].graph['node_feat'].to(agent.device), data[0].graph['edge_index'].to(agent.device))).detach()\n",
    "def entropy(pred):\n",
    "    return -(pred * torch.log(pred)).sum(dim=1)\n",
    "entropy_train = entropy(pred)\n",
    "\n",
    "entropys = []\n",
    "for data_test in data[2]:\n",
    "    pred_temp = F.softmax(agent.model.predict(data_test.graph['node_feat'].to(agent.device), data_test.graph['edge_index'].to(agent.device))).detach()\n",
    "    entropy_temp = entropy(pred_temp[data_test.test_mask] if hasattr(data_test, \"test_mask\") else pred_temp)\n",
    "    entropy_temp[entropy_temp.isnan()] = 0\n",
    "    entropys.append(entropy_temp)\n",
    "# entropy_test = torch.stack(entropys)\n",
    "print(\"Train mean entropy:\", entropy_train.mean())\n",
    "print(\"Test mean entropy:\", [x.mean() for x in entropys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.6272, device='cuda:0', grad_fn=<SubBackward0>)\n",
      "[-0.6123806238174438, -0.5777472257614136, -0.5543553829193115]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yy/miniconda3/envs/torch1121/lib/python3.8/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'dropout_adj' is deprecated, use 'dropout_edge' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "# preliminary: loss value\n",
    "data_temp = data[0]\n",
    "feat, labels = data_temp.graph['node_feat'].to(agent.device), data_temp.label.to(agent.device)#.squeeze()\n",
    "edge_index = data_temp.graph['edge_index'].to(agent.device)\n",
    "agent.edge_index, agent.feat, agent.labels = edge_index, feat, labels\n",
    "train_loss = agent.test_time_loss(agent.model, data_temp.graph['node_feat'].to(agent.device), data_temp.graph['edge_index'].to(agent.device), mode='eval')\n",
    "print(train_loss)\n",
    "test_losses = []\n",
    "for data_temp in data[-1]:\n",
    "    feat, labels = data_temp.graph['node_feat'].to(agent.device), data_temp.label.to(agent.device)#.squeeze()\n",
    "    edge_index = data_temp.graph['edge_index'].to(agent.device)\n",
    "    agent.edge_index, agent.feat, agent.labels = edge_index, feat, labels\n",
    "    test_loss = agent.test_time_loss(agent.model, data_temp.graph['node_feat'].to(agent.device), data_temp.graph['edge_index'].to(agent.device), mode='eval')\n",
    "    test_losses.append(test_loss.item())\n",
    "print(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accs: [0.4077676746317051, 0.39058566382389104, 0.348785054420509]\n",
      "flatten test: 0.37854279430345195\n"
     ]
    }
   ],
   "source": [
    "accs = agent.evaluate(agent.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: PearsonRResult(statistic=-0.7772486794327934, pvalue=0.02322074676564532)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "print(\"Correlation:\", scipy.stats.pearsonr(entropy_test.mean(dim=1).cpu(), accs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
